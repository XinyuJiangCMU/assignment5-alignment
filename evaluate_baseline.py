import os
import json
import torch
from vllm import LLM, SamplingParams
from tqdm import tqdm

# --- 1. é…ç½®è·¯å¾„ (è¯·ä¿®æ”¹ä¸ºä½ è‡ªå·±çš„çœŸå®è·¯å¾„) ---
# æ¨¡å‹ç»å¯¹è·¯å¾„ (åˆšæ‰ç”¨ python å‘½ä»¤æŸ¥åˆ°çš„é‚£ä¸ª)
MODEL_PATH = "/home/jxy/.cache/modelscope/hub/Qwen/Qwen2.5-Math-1.5B" 

# æ•°æ®é›†è·¯å¾„
DATA_PATH = "data/MATH/validation.jsonl"

# ç»“æœä¿å­˜è·¯å¾„
OUTPUT_FILE = "baseline_results.jsonl"

# --- 2. å¯¼å…¥ä½œä¸šè‡ªå¸¦çš„è¯„åˆ†å·¥å…· ---
# ç¡®ä¿ä½ åœ¨ assignment5-alignment æ ¹ç›®å½•ä¸‹è¿è¡Œï¼Œå¦åˆ™æ‰¾ä¸åˆ°è¿™ä¸ªåŒ…
try:
    from cs336_alignment.drgrpo_grader import r1_zero_reward_fn
except ImportError:
    print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° cs336_alignment æ¨¡å—ã€‚è¯·ç¡®ä¿ä½ åœ¨ä½œä¸šæ ¹ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬ã€‚")
    exit(1)

def load_data(filepath):
    prompts = []
    ground_truths = []
    print(f"æ­£åœ¨è¯»å–æ•°æ®: {filepath}")
    with open(filepath, "r", encoding="utf-8") as f:
        for line in f:
            item = json.loads(line)
            # ä½œä¸šæä¾›çš„ R1-Zero Prompt æ¨¡ç‰ˆæ–‡ä»¶
            # æˆ‘ä»¬ç›´æ¥æ‰‹åŠ¨æ„å»ºï¼Œé˜²æ­¢æ–‡ä»¶è¯»å–è·¯å¾„é”™è¯¯
            # æ ¼å¼å‚è€ƒ: cs336_alignment/prompts/r1_zero.prompt
            prompt_content = (
                "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. "
                "The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. "
                "The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, "
                "i.e., <think> reasoning process here </think> <answer> answer here </answer>.\n"
                f"User: {item['question']}\n"
                "Assistant: <think>"
            )
            prompts.append(prompt_content)
            ground_truths.append(item['answer'])
    return prompts, ground_truths

def main():
    # 1. å‡†å¤‡æ•°æ®
    if not os.path.exists(DATA_PATH):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {DATA_PATH}ã€‚è¯·æ£€æŸ¥ç¬¬ä¸€æ­¥æ˜¯å¦ä¸‹è½½æˆåŠŸã€‚")
        return
    
    prompts, ground_truths = load_data(DATA_PATH)
    print(f"åŠ è½½äº† {len(prompts)} æ¡æµ‹è¯•æ•°æ®ã€‚")

    # 2. åˆå§‹åŒ– vLLM
    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_PATH} ...")
    llm = LLM(
        model=MODEL_PATH,
        trust_remote_code=True,
        tensor_parallel_size=1, # å•å¡è¿è¡Œã€‚å¦‚æœä½ æƒ³ç”¨2å¼ å¡æ¨ç†ï¼Œæ”¹æˆ2
        gpu_memory_utilization=0.90, # æ˜¾å­˜å ç”¨ç‡
        dtype="bfloat16" # 3090 å¿…é¡»ç”¨ bf16
    )

    # 3. è®¾ç½®é‡‡æ ·å‚æ•° (ä½œä¸šè¦æ±‚)
    sampling_params = SamplingParams(
        temperature=1.0,
        top_p=1.0,
        max_tokens=1024,
        stop=["</answer>"], # å…³é”®ï¼šé‡åˆ°ç­”æ¡ˆç»“æŸæ ‡ç­¾åœæ­¢
        include_stop_str_in_output=True # ä¿ç•™ </answer> ä»¥ä¾¿è§£æ
    )

    # 4. æ‰§è¡Œæ¨ç† (Batch Inference)
    print("ğŸš€ å¼€å§‹æ¨ç† (Generating)...")
    outputs = llm.generate(prompts, sampling_params)

    # 5. è¯„åˆ†ä¸ç»Ÿè®¡
    print("æ­£åœ¨è¯„åˆ† (Grading)...")
    results = []
    correct_count = 0
    format_error_count = 0

    for i, output in tqdm(enumerate(outputs), total=len(outputs)):
        generated_text = output.outputs[0].text
        ground_truth = ground_truths[i]
        
        # æ‹¼æ¥å®Œæ•´çš„ç”Ÿæˆå†…å®¹ (Promptæœ€åçš„ <think> + ç”Ÿæˆçš„å†…å®¹)
        # æ³¨æ„ï¼švLLM ç”Ÿæˆçš„æ˜¯ Assistant åé¢çš„å†…å®¹ï¼Œæˆ‘ä»¬éœ€è¦æŠŠæ ‡ç­¾è¡¥å…¨æ–¹ä¾¿è§£æ
        # å…¶å® r1_zero_reward_fn åªéœ€è¦ç”Ÿæˆéƒ¨åˆ†çš„ string
        # ä½†ä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬æŠŠç”Ÿæˆçš„ç›´æ¥ä¼ è¿›å»ï¼Œä½œä¸šçš„ grader åº”è¯¥èƒ½å¤„ç†
        
        # è°ƒç”¨ä½œä¸šè‡ªå¸¦çš„è¯„åˆ†å‡½æ•°
        # è¾“å…¥ï¼šæ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ï¼Œæ ‡å‡†ç­”æ¡ˆ
        # è¾“å‡ºï¼š{'reward': 1.0/0.0, 'format_reward': 1/0, 'answer_reward': 1/0}
        
        # è¿™é‡Œæœ‰ä¸ªå°å‘ï¼šä½œä¸šæç¤ºè¯æœ«å°¾æ˜¯ "Assistant: <think>"
        # æ¨¡å‹ç”Ÿæˆçš„æ˜¯ "xxxx </think> <answer> yyyy </answer>"
        # ä¸ºäº†è®©è§£æå™¨å·¥ä½œï¼Œæˆ‘ä»¬æœ€å¥½æŠŠå¼€å¤´çš„ "<think>" è¡¥å›å»ä¼ ç»™ grader
        full_response = "<think>" + generated_text
        
        score = r1_zero_reward_fn(full_response, ground_truth)
        
        if score['reward'] == 1.0:
            correct_count += 1
        if score['format_reward'] == 0.0:
            format_error_count += 1
            
        results.append({
            "question": prompts[i],
            "ground_truth": ground_truth,
            "generated": full_response,
            "score": score
        })

    # 6. ä¿å­˜ç»“æœä¸è¾“å‡ºæŠ¥å‘Š
    accuracy = correct_count / len(prompts)
    format_error_rate = format_error_count / len(prompts)
    
    print("-" * 30)
    print(f"âœ… è¯„ä¼°å®Œæˆï¼")
    print(f"å‡†ç¡®ç‡ (Accuracy): {accuracy:.2%}")
    print(f"æ ¼å¼é”™è¯¯ç‡ (Format Error): {format_error_rate:.2%}")
    print(f"è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_FILE}")
    print("-" * 30)

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        for item in results:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

if __name__ == "__main__":
    main()