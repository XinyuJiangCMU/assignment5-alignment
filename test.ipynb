{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf94b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu124\n",
      "CUDA Available: True\n",
      "GPU Count: 4\n",
      "âœ… Flash Attention 2 installed successfully!\n",
      "INFO 12-07 13:41:06 __init__.py:190] Automatically detected platform cuda.\n",
      "âœ… vLLM installed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "\n",
    "try:\n",
    "    import flash_attn\n",
    "    print(\"âœ… Flash Attention 2 installed successfully!\")\n",
    "except ImportError:\n",
    "    print(\"âŒ Flash Attention 2 NOT found.\")\n",
    "\n",
    "try:\n",
    "    import vllm\n",
    "    print(\"âœ… vLLM installed successfully!\")\n",
    "except ImportError:\n",
    "    print(\"âŒ vLLM NOT found.\")\n",
    "\n",
    "# PyTorch Version: 2.5.1+cu124\n",
    "# CUDA Available: True\n",
    "# GPU Count: 4\n",
    "# âœ… Flash Attention 2 installed successfully!\n",
    "# INFO 12-07 13:41:06 __init__.py:190] Automatically detected platform cuda.\n",
    "# âœ… vLLM installed successfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b24ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š æ‹¿åˆ° 5000 é“åŸå§‹é¢˜ç›®ã€‚\n",
      "ğŸ–¨ï¸  æ­£åœ¨å°åˆ¶ R1-Zero è€ƒå·...\n",
      "\n",
      "========================================\n",
      "ğŸ” æŸ¥çœ‹ç¬¬ä¸€å¼ å°å¥½çš„è€ƒå· (Sample 0):\n",
      "--------------------\n",
      "A conversation between User and Assistant. The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>.\n",
      "User: How many vertical asymptotes does the graph of $y=\\frac{2}{x^2+x-6}$ have?\n",
      "Assistant: <think>\n",
      "========================================\n",
      "âœ… æ ¼å¼æ£€æŸ¥é€šè¿‡ï¼è¯•å·æœ«å°¾æˆåŠŸå¼ºåˆ¶äº† '<think>'ã€‚\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# å¤ç”¨ç¬¬ä¸€æ­¥çš„è·¯å¾„\n",
    "DATA_PATH = \"data/MATH/validation.jsonl\"\n",
    "\n",
    "# --- 1. å®šä¹‰ R1-Zero çš„æ¨¡ç‰ˆ (è¿™å°±æ˜¯â€œå°åˆ¶è¯•å·â€çš„æ¨¡å…·) ---\n",
    "# å‚è€ƒä½œä¸šæ–‡æ¡£ Page 4 çš„ r1_zero prompt [cite: 78]\n",
    "def apply_r1_zero_template(raw_question):\n",
    "    \"\"\"\n",
    "    è¾“å…¥ï¼šåŸå§‹é¢˜ç›®æ–‡æœ¬ (ä¾‹å¦‚ \"1+1=?\")\n",
    "    è¾“å‡ºï¼šåŒ…è£…å¥½çš„ Prompt å­—ç¬¦ä¸²\n",
    "    \"\"\"\n",
    "    template = (\n",
    "        \"A conversation between User and Assistant. The User asks a question, and the Assistant solves it. \"\n",
    "        \"The Assistant first thinks about the reasoning process in the mind and then provides the User with the answer. \"\n",
    "        \"The reasoning process is enclosed within <think> </think> and answer is enclosed within <answer> </answer> tags, respectively, \"\n",
    "        \"i.e., <think> reasoning process here </think> <answer> answer here </answer>.\\n\"\n",
    "        f\"User: {raw_question}\\n\"\n",
    "        \"Assistant: <think>\"  # <--- å…³é”®ç‚¹ï¼šè¿™ä¸€è¡Œé€¼ç€æ¨¡å‹å¼€å§‹æ€è€ƒ [cite: 82]\n",
    "    )\n",
    "    return template\n",
    "\n",
    "# --- 2. åŠ è½½æ•°æ®çš„ç®€æ˜“ç‰ˆ (å¤ç”¨ä¸Šä¸€æ­¥é€»è¾‘) ---\n",
    "def load_raw_questions(filepath):\n",
    "    qs = []\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                qs.append(json.loads(line)[\"question\"])\n",
    "    return qs\n",
    "\n",
    "# --- ä¸»ç¨‹åº ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. å…ˆæŠŠé¢˜ç›®æ‹¿å‡ºæ¥\n",
    "    raw_questions = load_raw_questions(DATA_PATH)\n",
    "    print(f\"ğŸ“š æ‹¿åˆ° {len(raw_questions)} é“åŸå§‹é¢˜ç›®ã€‚\")\n",
    "\n",
    "    # 2. å¼€å§‹â€œå°å·å­â€ (æ‰¹é‡å¤„ç†)\n",
    "    formatted_prompts = []\n",
    "    print(\"ğŸ–¨ï¸  æ­£åœ¨å°åˆ¶ R1-Zero è€ƒå·...\")\n",
    "    \n",
    "    for q in raw_questions:\n",
    "        # æ¯ä¸€ä¸ªé¢˜ç›®éƒ½å¡è¿›æ¨¡å…·é‡Œ\n",
    "        prompt = apply_r1_zero_template(q)\n",
    "        formatted_prompts.append(prompt)\n",
    "\n",
    "    # --- 3. æ£€æŸ¥æˆæœ (Visual Check) ---\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"ğŸ” æŸ¥çœ‹ç¬¬ä¸€å¼ å°å¥½çš„è€ƒå· (Sample 0):\")\n",
    "    print(\"-\" * 20)\n",
    "    # æ‰“å°å‡ºæ¥çœ‹çœ‹ï¼Œä½ ä¼šå‘ç°å®ƒæ¯”åŸå§‹é¢˜ç›®é•¿äº†å¾ˆå¤š\n",
    "    print(formatted_prompts[0]) \n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # éªŒè¯ä¸€ä¸‹ç»“å°¾æ˜¯ä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„\n",
    "    if formatted_prompts[0].endswith(\"Assistant: <think>\"):\n",
    "        print(\"âœ… æ ¼å¼æ£€æŸ¥é€šè¿‡ï¼è¯•å·æœ«å°¾æˆåŠŸå¼ºåˆ¶äº† '<think>'ã€‚\")\n",
    "    else:\n",
    "        print(\"âŒ æ ¼å¼é”™è¯¯ï¼æœ«å°¾æ²¡æœ‰è¯±å¯¼æ ‡ç­¾ã€‚\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
